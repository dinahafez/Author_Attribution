{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pt\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from os import walk\n",
    "from os import path\n",
    "from pprint import pprint\n",
    "from scipy.stats import sem # standard error of mean\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest, chi2, f_classif, f_regression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model.stochastic_gradient import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier, _predict_binary\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from random import randint\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from syllables_en import count\n",
    "from sys import maxint\n",
    "from time import time\n",
    "from scipy.cluster.vq import whiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "#setting some variables\n",
    "NUMFOLDS = 10\n",
    "RANGE = 25 # set to 25 based on Diederich et al. 2000 as cited on page 9 of http://www.cnts.ua.ac.be/stylometry/Papers/MAThesis_KimLuyckx.pdf\n",
    "SRCDIR = '' #path.dirname(path.realpath(__file__))\n",
    "FEATURESFILE = path.join(SRCDIR,'bookfeatures.txt')\n",
    "PICKLEFILE = path.join(SRCDIR,'estimator.pickle')\n",
    "CORPUSPATH = path.join(SRCDIR,'../corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyFreqDist(FreqDist):\n",
    "    '''\n",
    "    Extend FreqDist to implement dis legomena\n",
    "    '''\n",
    "\n",
    "    def dises(self):\n",
    "        '''\n",
    "        @return: A list of all samples that occur twice (dis legomena)\n",
    "        @rtype: C{list}\n",
    "        '''\n",
    "\n",
    "        return [item for item in self if self[item] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_pron_set():\n",
    "    '''\n",
    "    Build set of nominative pronouns.\n",
    "    '''\n",
    "\n",
    "    return set(open(path.join(SRCDIR,'nompronouns.txt'), 'r').read().splitlines())\n",
    "\n",
    "def build_conj_set():\n",
    "    '''\n",
    "    Build set of coordinating and subordinating conjunctions.\n",
    "    '''\n",
    "\n",
    "    return set(open(path.join(SRCDIR,'coordconj.txt'), 'r').read().splitlines()).union(\n",
    "           set(open(path.join(SRCDIR,'subordconj.txt'), 'r').read().splitlines()))\n",
    "\n",
    "def build_stop_words_set():\n",
    "    '''\n",
    "    Build set of stop words to ignore.\n",
    "    '''\n",
    "\n",
    "    # source: http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop\n",
    "    return set(open(path.join(SRCDIR,'smartstop.txt'), 'r').read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_dir_list(dir):\n",
    "    '''\n",
    "    Get a list of directories and files. Used to get the corpora.\n",
    "    Returns\n",
    "    -------\n",
    "    dir_list: list of directory names to serve as class labels.\n",
    "    file_list: list of files in corpus.\n",
    "    '''\n",
    "\n",
    "    file_list = []\n",
    "    dir_list = []\n",
    "    for (dirpath, dirname, files) in walk(dir):\n",
    "        if files:\n",
    "            dir_list.append(path.split(dirpath)[1])\n",
    "            file_list.append(map(lambda x: path.join(dirpath, x), files))\n",
    "    return dir_list, file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_book_contents(text):\n",
    "    '''\n",
    "    Extract the contents of the book after excising the Project Gutenber headers\n",
    "    and footers.\n",
    "    '''\n",
    "\n",
    "    start  = re.compile('START OF.*\\r\\n')\n",
    "    end = re.compile('\\*\\*.*END OF ([THIS]|[THE])')\n",
    "    # remove PG header and footer\n",
    "    _1 = re.split(start, text)\n",
    "    _2 = re.split(end, _1[1])\n",
    "    return _2[0] # lower-case everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_book_chapters2(filename):\n",
    "    '''\n",
    "    Extract the chapters of the book after excising the Project Gutenber headers\n",
    "    and footers.\n",
    "    '''\n",
    "    text = extract_book_contents(open(filename, 'r').read().decode('utf-8')).lower()\n",
    "      \n",
    "    romans = re.compile ('\\n[clxvi]+\\.')\n",
    "    #romans2 = re.compile('\\n(?=[mdclxvi])m*d?c{0,4}l?x{0,4}v?i{0,4}\\.')\n",
    "    numbers = re.compile ('\\n[0-9]{1,3}[\\.\\)]+') \n",
    "    chapter_delim = re.compile('\\nchapter[ \\t]*[0-9]{1,3}')  \n",
    "    chapter_delim2 = re.compile('\\nchapter[ \\t]*[mdclxvi]{1,3}') \n",
    "    play_delim = re.compile('scene[ \\t]*[mdclxvi]{1,3}') \n",
    "    \n",
    "    c1 = re.split(numbers, text)\n",
    "    c2 = re.split(chapter_delim, text)\n",
    "    c3 = re.split(romans, text)\n",
    "    c4 = re.split(chapter_delim2, text)\n",
    "    c5 = re.split(play_delim, text)\n",
    "\n",
    "    c_num = max(len(c1),len(c2),len(c3),len(c4), len(c5))\n",
    "    if (len(c1) == c_num):\n",
    "        chapters = c1\n",
    "    elif (len(c2) == c_num):\n",
    "        chapters = c2\n",
    "    elif (len(c3) == c_num):\n",
    "        chapters = c3\n",
    "    elif (len(c4) == c_num):\n",
    "        chapters = c4\n",
    "    elif (len(c5) == c_num):\n",
    "        chapters = c5\n",
    "    #import pdb;pdb.set_trace()\n",
    "    #If the book couldn't be split into chapters, return it all\n",
    "    #else, remove the first chapter which is the table of contents or introduction\n",
    "    if(len(chapters) == 1):\n",
    "        return chapters\n",
    "    else:\n",
    "        return chapters[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the main function that genetrates necessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_book_features_by_chapters(filename, smartStopWords={}, pronSet={}, conjSet={}):\n",
    "    '''\n",
    "    Load features for each book in the corpus. There are 4 + RANGE*4 features\n",
    "    for each instance. These features are:\n",
    "       ---------------------------------------------------------------------------------------------------------\n",
    "       No. Feature Name                                                                         No. of features.\n",
    "       ---------------------------------------------------------------------------------------------------------\n",
    "       1.  number of hapax legomena divided by number of unique words                           1\n",
    "       2.  number of dis legomena divided by number of unique words                             1\n",
    "       3.  number of unique words divided by number of total words                              1\n",
    "       4.  flesch readability score divided by 100                                              1\n",
    "\n",
    "       5.  no. of sentences of length in the range [1, 25] divided by the                       25\n",
    "           number of total sentences\n",
    "       6.  no. of words of length in the range [1, 25] divided by the                           25\n",
    "           number of total words\n",
    "       7.  no. of nominative pronouns per sentence in the range [1, 25] divided by the          25\n",
    "           number of total sentences\n",
    "       8.  no. of (coordinating + subordinating) conjunctions per sentence in the range         25\n",
    "           [1, 25] divided by the number of total sentences\n",
    "       9.  Average number of words per sentence                                                  1\n",
    "       10. Sentence length variation                                                             1\n",
    "       11. Lexical diversity                                                                     1\n",
    "       12. Number of Commas per sentence                                                         1\n",
    "       13. Semicolons per sentence                                                               1\n",
    "       14. Number of Colons per sentence                                                         1\n",
    "       15. Number of Exclamation marks per sentence                                              1\n",
    "       16. Bag of words features (most common 30 words in the whole text )                       30\n",
    "       17. Count for Part of Speech (POS) representation of ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS'] 6\n",
    "        Total feature count = 147\n",
    "    '''\n",
    "    chapters = extract_book_chapters2(filename)\n",
    "    all_text = ' '.join(chapters)\n",
    "    features_token_level =  np.zeros((len(chapters), 111), np.float64) #np.zeros((1, 104), np.float64)\n",
    "    for e, ch_text in enumerate(chapters):\n",
    "        if (len(ch_text)>100):\n",
    "            # note: the nltk.word_tokenize includes punctuation\n",
    "            #ch_text = ch_text_.decode('utf-8')\n",
    "            contents = re.sub('\\'s|(\\r\\n)|-+|[\"_]', ' ', ch_text) # remove \\r\\n, apostrophes, and dashes\n",
    "            sentenceList = sent_tokenize(contents.strip())\n",
    "            cleanWords = []\n",
    "            sentenceLenDist = []\n",
    "            pronDist = []\n",
    "            conjDist = []\n",
    "            sentences = []\n",
    "            totalWords = 0\n",
    "            wordLenDist = []\n",
    "            totalSyllables = 0\n",
    "            for sentence in sentenceList:\n",
    "                if sentence != \".\":\n",
    "                    pronCount = 0\n",
    "                    conjCount = 0\n",
    "                    sentences.append(sentence)\n",
    "                    sentenceWords = re.findall(r\"[\\w']+\", sentence)\n",
    "                    totalWords += len(sentenceWords) # record all words in sentence\n",
    "                    sentenceLenDist.append(len(sentenceWords)) # record length of sentence in words\n",
    "                    for word in sentenceWords:\n",
    "                        totalSyllables += count(word)\n",
    "                        wordLenDist.append(len(word)) # record length of word in chars\n",
    "                        if word in pronSet:\n",
    "                            pronCount+=1 # record no. of pronouns in sentence\n",
    "                        if word in conjSet:\n",
    "                            conjCount+=1 # record no. of conjunctions in sentence\n",
    "                        if word not in smartStopWords:\n",
    "                            cleanWords.append(word)\n",
    "                    pronDist.append(pronCount)\n",
    "                    conjDist.append(conjCount)\n",
    "\n",
    "            sentenceLengthFreqDist = FreqDist(sentenceLenDist)\n",
    "            sentenceLengthDist = map(lambda x: sentenceLengthFreqDist.freq(x), range(1, RANGE))\n",
    "            sentenceLengthDist.append(1-sum(sentenceLengthDist))\n",
    "\n",
    "            pronounFreqDist = FreqDist(pronDist)\n",
    "            pronounDist = map(lambda x: pronounFreqDist.freq(x), range(1, RANGE))\n",
    "            pronounDist.append(1-sum(pronounDist))\n",
    "\n",
    "            conjunctionFreqDist = FreqDist(conjDist)\n",
    "            conjunctionDist = map(lambda x: conjunctionFreqDist.freq(x), range(1, RANGE))\n",
    "            conjunctionDist.append(1-sum(conjunctionDist))\n",
    "\n",
    "            wordLengthFreqDist= FreqDist(wordLenDist)\n",
    "            wordLengthDist = map(lambda x: wordLengthFreqDist.freq(x), range(1, RANGE))\n",
    "            wordLengthDist.append(1-sum(wordLengthDist))\n",
    "\n",
    "            # calculate readability\n",
    "            avgSentenceLength = np.mean(sentenceLenDist)\n",
    "            avgSyllablesPerWord = float(totalSyllables)/totalWords\n",
    "            readability = float(206.835 - (1.015 * avgSentenceLength) - (84.6 * avgSyllablesPerWord))/100\n",
    "\n",
    "            wordsFreqDist = MyFreqDist(FreqDist(cleanWords))\n",
    "            #sentenceDist = FreqDist(sentences)\n",
    "            #print sentenceDist.keys()[:15] # most common sentences\n",
    "            #print wordsFreqDist.keys()[:15] # most common words\n",
    "            #print wordsFreqDist.keys()[-15:] # most UNcommon words\n",
    "\n",
    "            numUniqueWords = len(wordsFreqDist.keys())\n",
    "            numTotalWords = len(cleanWords)\n",
    "\n",
    "            hapax = float(len(wordsFreqDist.hapaxes()))/numUniqueWords # no. words occurring once / total num. UNIQUE words\n",
    "            dis = float(len(wordsFreqDist.dises()))/numUniqueWords # no. words occurring twice / total num. UNIQUE words\n",
    "            richness = float(numUniqueWords)/numTotalWords # no. unique words / total num. words\n",
    "\n",
    "            result = []\n",
    "            result.append(hapax)\n",
    "            result.append(dis)\n",
    "            result.append(richness)\n",
    "            result.append(readability)\n",
    "            result.extend(sentenceLengthDist)\n",
    "            result.extend(wordLengthDist)\n",
    "            result.extend(pronounDist)\n",
    "            result.extend(conjunctionDist)\n",
    "\n",
    "            #some more lexical features\n",
    "            # note: the nltk.word_tokenize includes punctuation\n",
    "            tokens = word_tokenize(ch_text.lower())\n",
    "            words = word_tokenizer.tokenize(ch_text.lower())\n",
    "            sentences = sentence_tokenizer.tokenize(ch_text)\n",
    "            vocab = set(words)\n",
    "            words_per_sentence = np.array([len(word_tokenizer.tokenize(s))\n",
    "                                           for s in sentences])\n",
    "\n",
    "            # average number of words per sentence\n",
    "            result.append(words_per_sentence.mean())\n",
    "            # sentence length variation\n",
    "            result.append(words_per_sentence.std())\n",
    "            # Lexical diversity\n",
    "            result.append(len(vocab) / float(len(words)))\n",
    "\n",
    "            # Commas per sentence\n",
    "            result.append(tokens.count(',') / float(len(sentences)))\n",
    "            # Semicolons per sentence\n",
    "            result.append(tokens.count(';') / float(len(sentences)))\n",
    "            # Colons per sentence\n",
    "            result.append(tokens.count(':') / float(len(sentences)))\n",
    "            # Exclamation marks per sentence\n",
    "            result.append(tokens.count('!') / float(len(sentences)))\n",
    "\n",
    "            apply whitening to decorrelate the features for normalization divide by std\n",
    "            fvs_lexical = whiten(fvs_lexical)\n",
    "            fvs_punct = whiten(fvs_punct)     \n",
    "            features_token_level[e] = result \n",
    "        else:\n",
    "            np.delete(features_token_level, e, 0)\n",
    "    #Bag of words features (most common words in the whole text )\n",
    "    bow = BagOfWords(all_text, chapters)  \n",
    "    allfeatures1 = np.concatenate((features_token_level, bow), axis=1)\n",
    "    sf = SyntacticFeatures(chapters)\n",
    "    allfeatures = np.concatenate((allfeatures1, sf), axis=1)\n",
    "    return allfeatures, len(chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BagOfWords(all_text, chapters):\n",
    "    \"\"\"\n",
    "    Compute the bag of words feature vectors, based on the most common words\n",
    "     in the whole book\n",
    "    \"\"\"\n",
    "    # get most common words in the whole book\n",
    "    NUM_TOP_WORDS = 30\n",
    "    all_tokens = nltk.word_tokenize(all_text)\n",
    "    fdist = nltk.FreqDist(all_tokens)\n",
    "    vocab = fdist.keys()[:NUM_TOP_WORDS]\n",
    "\n",
    "    # use sklearn to create the bag for words feature vector for each chapter\n",
    "    vectorizer = CountVectorizer(vocabulary=vocab, tokenizer=nltk.word_tokenize)\n",
    "    fvs_bow = vectorizer.fit_transform(chapters).toarray().astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by its Euclidean norm\n",
    "    fvs_bow /= np.c_[np.apply_along_axis(np.linalg.norm, 1, fvs_bow)]\n",
    "\n",
    "    return fvs_bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SyntacticFeatures(chapters):\n",
    "    \"\"\"\n",
    "    Extract feature vector for part of speech frequencies\n",
    "    \"\"\"\n",
    "    def token_to_pos(ch):\n",
    "        tokens = nltk.word_tokenize(ch)\n",
    "        return [p[1] for p in nltk.pos_tag(tokens)]\n",
    "\n",
    "    chapters_pos = [token_to_pos(ch) for ch in chapters]\n",
    "    pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "    fvs_syntax = np.array([[ch.count(pos) for pos in pos_list]\n",
    "                           for ch in chapters_pos]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens in the chapter\n",
    "    fvs_syntax /= np.c_[np.array([len(ch) for ch in chapters_pos])]\n",
    "\n",
    "    return fvs_syntax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_book_features_to_file(x, y, le):\n",
    "    '''\n",
    "    Save book features to a features file.\n",
    "    '''\n",
    "\n",
    "    f = open(FEATURESFILE, 'wb')\n",
    "    for index, item in enumerate(x):\n",
    "        f.write(\"%s\\t%d\\t%s\\n\" % (le.inverse_transform(y[index]), y[index], ', '.join(map(str, item))))\n",
    "    f.close()\n",
    "\n",
    "    print 'Features saved to file %s' % FEATURESFILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_book_features_from_corpus(dir_list, file_list, smartStopWords={}, pronSet={}, conjSet={}):\n",
    "    '''\n",
    "    Parse each book and load its features.\n",
    "    '''\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    t0 = time()\n",
    "    totalwords = 0\n",
    "    for index, files in enumerate(file_list):\n",
    "        for f in files:\n",
    "            print index\n",
    "            print f\n",
    "           \n",
    "            features, numChapters = load_book_features_by_chapters(f, smartStopWords, pronSet, conjSet)\n",
    "        \n",
    "            x.extend(features)\n",
    "            for l in range(0, numChapters):\n",
    "                y.append(dir_list[index])\n",
    "            #import pdb;pdb.set_trace()\n",
    "    le = LabelEncoder().fit(y)\n",
    "    #print 'Processed %d books from %d authors with %d total words in %2.3fs' % (len(x), len(dir_list), totalwords, time()-t0)\n",
    "    return np.array(x), np.array(le.transform(y)), le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def run_feature_extraction():\n",
    "'''\n",
    "Initiate feature_extraction.\n",
    "'''\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "if not path.exists(FEATURESFILE):\n",
    "    print 'Feature file not found. Creating...'\n",
    "    pronSet = build_pron_set()\n",
    "    conjSet = build_conj_set()\n",
    "    smartStopWords = build_stop_words_set()\n",
    "\n",
    "    dir_list, file_list = get_file_dir_list(CORPUSPATH)\n",
    "\n",
    "    x, y, le = load_book_features_from_corpus(dir_list, file_list, smartStopWords, pronSet, conjSet)\n",
    "    \n",
    "    #import pdb;pdb.set_trace()\n",
    "    save_book_features_to_file(x, y, le)\n",
    "    print '... done.'\n",
    "    print\n",
    "#else:\n",
    "#    print 'Feature file found. Reading...'\n",
    "#    print\n",
    "#    x, y = load_book_features_from_file()\n",
    "#    import pdb;pdb.set_trace()\n",
    "no_samples = x.shape[0]\n",
    "no_classes = len(set(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
